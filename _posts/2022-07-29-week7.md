Parameter optimization is the process of selecting the coefficients, weights, or other parameters that produce the best results for a given algorithm. For the Needleman-Wunsch algorithm with an affine gap penalty, those parameters are match, mismatch, gap initialization, and gap extension scores. For our modified algorithm, this includes an additional parameter that accounts for the quality of each query sequence base, and increases the likelihood of introducing a mismatch (rather than a gap) if the quality is poor. 

This week, my goal was to begin tuning these parameters. However, I discovered a bug in my multithreading process that caused a rare problem with the alignment. One out of every ~100,000 alignments was not reproducible, which meant I likely had a memory collision somewhere. 

I spent the remainder of the week tracking down and fixing the collision issue, and then set up a revised time at the beginning of week eight to work on parameter tuning. 
